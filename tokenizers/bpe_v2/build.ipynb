{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eba617fb-d786-4169-8d39-2a97145c9245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fed68fd-7c03-4d2e-92e4-dd3a2d0d7fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading a sample from our TinyStories dataset\n",
    "from tools import get_data_loader\n",
    "data_loader = get_data_loader(batch_size=5_000, split='train')\n",
    "\n",
    "batch = next(iter(data_loader))\n",
    "print(len(batch), batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2133207-6ee0-4f47-8018-da1a867c5136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn it into one string instead of a list of strings\n",
    "combined_string = '\\n\\n'.join(batch)\n",
    "\n",
    "# find the unique characters\n",
    "chars = sorted(list(set(combined_string)))\n",
    "# this is the largest set of characters i found earlier from a batch size of 1_000_000, total 95 characters\n",
    "if len(chars) < 95:\n",
    "    chars = ['\\t', '\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~']\n",
    "v = len(chars)\n",
    "print('\\n', chars, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3856169e-9b04-440b-9119-70832902bc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "char_encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca80d95-7491-4ffb-9bbc-63e521306590",
   "metadata": {},
   "source": [
    "# Regex\n",
    "this is a pre-processing stage where we set the rules for what types of characters are allowed to be merged together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5da62b54-17dd-48a0-bab4-f9b1fd07413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d3dad59-9791-4588-96c3-134a2d8390a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't ask me the specifics of how this plays out, i just know it's what they used for GPT4\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "# if you want to mess around with building your own tokenizer, then ^this string is one of the things to mess around with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04f316b-21a9-4bdc-a7ed-c622eb1b4e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_pattern = re.compile(GPT4_SPLIT_PATTERN)\n",
    "print(compiled_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c183a0fc-400d-41ed-9272-1d7a96e08247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the text up into text chunks\n",
    "text_chunks = re.findall(compiled_pattern, combined_string)\n",
    "print(len(combined_string), len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dad461b-9959-463e-9a87-8934400ecb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_string[:100])\n",
    "print(text_chunks[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa9c183-5eb1-4b32-a5f7-3e5813b1e100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input text preprocessing\n",
    "ids = [char_encode(ch) for ch in text_chunks] # list(ch.encode(\"utf-8\")) # <- use that instead to do actual bytez instead of characters\n",
    "ids_backup = ids # saving this for later just to see how much compression we get\n",
    "print(len(ids), ids[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b53373-b33d-4eba-b640-a50e3334045a",
   "metadata": {},
   "source": [
    "so this regex just splits the text up into all the token ids that are allowed to be merged, meaning that the regex output we saw above is an upper limit on the tokens that we could end up with if we get a large enough vocabulary, rather than a starting point. ngl the reason I didn't use regex on bpe_v1 is because I thought it was a starting point and didn't understand how subwords were supposed to work -_-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e216a550-4bbb-4d8e-ba57-25af48a9d65b",
   "metadata": {},
   "source": [
    "# CPE tokenization\n",
    "CPE = character-pair encoding instead of byte-pair encoding. Honestly we could've done BPE and the only difference would've been a negligibly larger embedding tensor in our model (specifically 256-95=161 extra rows) but might as well since we know we're only gonne be using this one same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba69cd54-8390-4355-881a-cd2f233922dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 16384 # the desired final vocabulary size\n",
    "num_merges = vocab_size - v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390137ee-0025-4515-8e32-fc9a46494b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most models work off bytes, but we'll be simplifying to just the index of each unique character\n",
    "base_indices = char_encode(chars)\n",
    "print(base_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0e7fcb6-5927-45b9-b6a9-23ce1df8f312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids, counts=None):\n",
    "    \"\"\"\n",
    "    Given a list of integers, return a dictionary of counts of consecutive pairs\n",
    "    Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n",
    "    Optionally allows to update an existing dictionary of counts\n",
    "    \"\"\"\n",
    "    counts = {} if counts is None else counts\n",
    "    for pair in zip(ids, ids[1:]): # iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    \"\"\"\n",
    "    In the list of integers (ids), replace all consecutive occurrences\n",
    "    of pair with the new integer token idx\n",
    "    Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n",
    "    \"\"\"\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # if not at the very last position AND the pair matches, replace it\n",
    "        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba933bfb-026c-47f7-8fff-42868bf5818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's actually do it\n",
    "merges = {} # (int, int) -> int\n",
    "for i in tqdm(range(num_merges)):\n",
    "    # count the number of times every consecutive pair appears\n",
    "    stats = {}\n",
    "    for chunk_ids in ids:\n",
    "        # passing in stats will update it in place, adding up counts\n",
    "        get_stats(chunk_ids, stats)\n",
    "    # find the pair with the highest count\n",
    "    pair = max(stats, key=stats.get)\n",
    "    # mint a new token: assign it the next available id\n",
    "    idx = v + i\n",
    "    # replace all occurrences of pair in ids with idx\n",
    "    ids = [merge(chunk_ids, pair, idx) for chunk_ids in ids]\n",
    "    # save the merge\n",
    "    merges[pair] = idx\n",
    "    #print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} had {stats[pair]} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea99684-f232-4a8f-80f4-48b841633ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "og = sum([len(t) for t in ids_backup])\n",
    "new = sum([len(t) for t in (ids)])\n",
    "print(\"original length:\", og) # remember tokens are our original tokens\n",
    "print(\"ids length:\", new) # and ids are new tokens we've made\n",
    "print(f\"compression ratio: {og / new:.2f}X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcfc996d-fc57-469d-9a25-bc441cc4242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the tokenizers directory exists\n",
    "if not os.path.exists('./models'):\n",
    "    os.makedirs('./models')\n",
    "\n",
    "# Prepare the tokenizer data to be saved\n",
    "tokenizer_data = {\n",
    "    'stoi': stoi,  # Character to integer mapping\n",
    "    'merges': merges  # Merges dictionary\n",
    "}\n",
    "\n",
    "# Save the tokenizer data using pickle\n",
    "with open(f'./models/{vocab_size}.model', 'wb') as f:\n",
    "    pickle.dump(tokenizer_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db232b16-94c5-4a41-ba7f-63c26bcfff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a pre-existing tokenizer and trimming it down to a smaller size\n",
    "# i basically ran this cell and then one above it multiple times until i got to the smallest possible size (128)\n",
    "vocab_size = vocab_size // 2 # len(chars) for character-wise tokenization\n",
    "merges = {k: v for k, v in merges.items() if v < vocab_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "030522f4-ad67-4200-8598-261a9e1fa744",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import load_tokenizer_data, BPE_Tokenizer\n",
    "vocab_size = 16384\n",
    "tokenizer_data = load_tokenizer_data(f'models/{vocab_size}.model')\n",
    "tokenizer = BPE_Tokenizer(tokenizer_data['stoi'], tokenizer_data['merges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857e3d5b-85b9-4ce7-bd41-f1f8de91cb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Once upon a time there was a boy named Tim.'\n",
    "print(tokenizer.display(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e223ca98-4c6a-471c-85e0-40c6b46b3a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.display(batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba18a30-c97c-41bc-a37b-23e82bd7d055",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(vocab_size):\n",
    "    print(f\"{i}: '{tokenizer.decode([i])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8967b6-f78b-4852-90f3-f3a1f37455cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
